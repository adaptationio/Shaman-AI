{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.sb3 import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-30 16:43:01,229]\u001b[0m A new study created in RDB with name: ppo299_sortino2\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 375`, after every 5 untruncated mini-batches, there will be a truncated mini-batch of size 55\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=375 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:43:17,711]\u001b[0m Trial 0 finished with value: -69.7 and parameters: {'n_steps': 375.96192885266925, 'gamma': 0.9655210634642647, 'learning_rate': 0.00015848728363978832, 'ent_coef': 6.774265231303632e-08, 'clip_range': 0.1995232886017248, 'n_epochs': 1.0043708215715677}. Best is trial 0 with value: -69.7.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1150`, after every 17 untruncated mini-batches, there will be a truncated mini-batch of size 62\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1150 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:43:45,403]\u001b[0m Trial 1 finished with value: -365.1 and parameters: {'n_steps': 1150.4439126991792, 'gamma': 0.9363397206393553, 'learning_rate': 0.002574101011476685, 'ent_coef': 3.4885903722619094e-05, 'clip_range': 0.16813216659076452, 'n_epochs': 3.222209434065739}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 168`, after every 2 untruncated mini-batches, there will be a truncated mini-batch of size 40\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=168 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:44:34,863]\u001b[0m Trial 2 finished with value: -103.0 and parameters: {'n_steps': 168.05291175376976, 'gamma': 0.9379473213670142, 'learning_rate': 0.004233350481921359, 'ent_coef': 2.0347519871112065e-06, 'clip_range': 0.3444226563171864, 'n_epochs': 22.98635257011294}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 95`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 31\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=95 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:44:55,431]\u001b[0m Trial 3 finished with value: -9.6 and parameters: {'n_steps': 95.54045020940998, 'gamma': 0.9848033164863629, 'learning_rate': 0.017577710729140914, 'ent_coef': 0.001057547848118625, 'clip_range': 0.23938879143552466, 'n_epochs': 2.751368884404474}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 45`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 45\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=45 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:45:18,996]\u001b[0m Trial 4 finished with value: -87.5 and parameters: {'n_steps': 45.89970610373743, 'gamma': 0.9699577977454258, 'learning_rate': 4.501019149617595e-05, 'ent_coef': 1.3957378959704511e-06, 'clip_range': 0.3858000994721352, 'n_epochs': 4.128084271577427}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 284`, after every 4 untruncated mini-batches, there will be a truncated mini-batch of size 28\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=284 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:45:39,813]\u001b[0m Trial 5 finished with value: -221.7 and parameters: {'n_steps': 284.3601872253276, 'gamma': 0.9907524178980552, 'learning_rate': 0.0024554885360287, 'ent_coef': 0.0007772476896073385, 'clip_range': 0.20474920858502335, 'n_epochs': 1.4946017340395088}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 37`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 37\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=37 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:46:14,665]\u001b[0m Trial 6 finished with value: -138.0 and parameters: {'n_steps': 37.37545931173557, 'gamma': 0.9500583847239149, 'learning_rate': 3.35466928683091e-05, 'ent_coef': 0.010929332618076628, 'clip_range': 0.30467431878531415, 'n_epochs': 8.15069726512057}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1971`, after every 30 untruncated mini-batches, there will be a truncated mini-batch of size 51\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1971 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:46:44,004]\u001b[0m Trial 7 finished with value: -51.5 and parameters: {'n_steps': 1971.9990420723675, 'gamma': 0.9043359442169454, 'learning_rate': 1.0406598985184325e-05, 'ent_coef': 4.2588836024150894e-05, 'clip_range': 0.2827126121074248, 'n_epochs': 7.377645942238957}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 17`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=17 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:48:31,698]\u001b[0m Trial 8 finished with value: -9.2 and parameters: {'n_steps': 17.36086864919424, 'gamma': 0.9115503448990826, 'learning_rate': 0.6756744242898002, 'ent_coef': 0.0017461115643412869, 'clip_range': 0.11470213835858782, 'n_epochs': 19.177986644584742}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 36`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=36 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:48:53,031]\u001b[0m Trial 9 finished with value: -124.3 and parameters: {'n_steps': 36.88779175174232, 'gamma': 0.9509590045921911, 'learning_rate': 0.0002581374671072895, 'ent_coef': 3.2466274568936396e-08, 'clip_range': 0.38134168063739404, 'n_epochs': 1.6462458085652782}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2047`, after every 31 untruncated mini-batches, there will be a truncated mini-batch of size 63\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2047 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-30 16:50:29,030]\u001b[0m Trial 10 finished with value: -9.3 and parameters: {'n_steps': 2047.5955765610604, 'gamma': 0.922780686741381, 'learning_rate': 0.12265437843834635, 'ent_coef': 2.4820350032650058e-05, 'clip_range': 0.11235873603905017, 'n_epochs': 47.40673532043894}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 594`, after every 9 untruncated mini-batches, there will be a truncated mini-batch of size 18\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=594 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:50:53,807]\u001b[0m Trial 11 finished with value: -149.2 and parameters: {'n_steps': 594.8419406475872, 'gamma': 0.9281308448569101, 'learning_rate': 0.0013746572057786502, 'ent_coef': 0.00014634722194711283, 'clip_range': 0.17400432928595921, 'n_epochs': 1.8976793038087894}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 743`, after every 11 untruncated mini-batches, there will be a truncated mini-batch of size 39\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=743 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:51:18,271]\u001b[0m Trial 12 finished with value: -144.3 and parameters: {'n_steps': 743.2531027523389, 'gamma': 0.99955011561738, 'learning_rate': 0.013751923171819095, 'ent_coef': 0.07119646181353802, 'clip_range': 0.17090776326014745, 'n_epochs': 1.136904964750979}. Best is trial 1 with value: -365.1.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1119`, after every 17 untruncated mini-batches, there will be a truncated mini-batch of size 31\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1119 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:51:45,313]\u001b[0m Trial 13 finished with value: -422.9 and parameters: {'n_steps': 1119.9072750899527, 'gamma': 0.9996734300341857, 'learning_rate': 0.0007868638742019481, 'ent_coef': 4.7176480359337634e-06, 'clip_range': 0.22329532529062732, 'n_epochs': 4.10739361874997}. Best is trial 13 with value: -422.9.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1139`, after every 17 untruncated mini-batches, there will be a truncated mini-batch of size 51\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1139 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:52:12,734]\u001b[0m Trial 14 finished with value: -326.2 and parameters: {'n_steps': 1139.610038001515, 'gamma': 0.9351740976509749, 'learning_rate': 0.00039933351647307175, 'ent_coef': 1.611172723413314e-06, 'clip_range': 0.1455559294826355, 'n_epochs': 4.206897678367815}. Best is trial 13 with value: -422.9.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1283`, after every 20 untruncated mini-batches, there will be a truncated mini-batch of size 3\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1283 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:52:41,372]\u001b[0m Trial 15 finished with value: -310.3 and parameters: {'n_steps': 1283.080309855598, 'gamma': 0.964890434293829, 'learning_rate': 0.02100364448305734, 'ent_coef': 8.753731778789284e-06, 'clip_range': 0.24574248020674128, 'n_epochs': 4.311247218095211}. Best is trial 13 with value: -422.9.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 707`, after every 11 untruncated mini-batches, there will be a truncated mini-batch of size 3\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=707 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n",
      "\u001b[32m[I 2021-07-30 16:53:21,251]\u001b[0m Trial 16 finished with value: -219.9 and parameters: {'n_steps': 707.659332229759, 'gamma': 0.9173935440105294, 'learning_rate': 0.0008123873258936673, 'ent_coef': 2.0785882866484473e-07, 'clip_range': 0.2159296557494984, 'n_epochs': 11.87289543378733}. Best is trial 13 with value: -422.9.\u001b[0m\n",
      "\u001b[32m[I 2021-07-30 16:53:51,415]\u001b[0m Trial 17 finished with value: -253.4 and parameters: {'n_steps': 2047.5840012824588, 'gamma': 0.9463648549780816, 'learning_rate': 0.004412649547758805, 'ent_coef': 0.00016395588409241977, 'clip_range': 0.2812504085237739, 'n_epochs': 3.3907358656711257}. Best is trial 13 with value: -422.9.\u001b[0m\n",
      "/home/adaptation/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py:138: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1220`, after every 19 untruncated mini-batches, there will be a truncated mini-batch of size 4\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1220 and n_envs=1)\n",
      "  f\"You have specified a mini-batch size of {batch_size},\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  19\n",
      "Best trial:\n",
      "Value:  -422.9\n",
      "Params: \n",
      "    clip_range: 0.22329532529062732\n",
      "    ent_coef: 4.7176480359337634e-06\n",
      "    gamma: 0.9996734300341857\n",
      "    learning_rate: 0.0007868638742019481\n",
      "    n_epochs: 4.10739361874997\n",
      "    n_steps: 1119.9072750899527\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_clip_range</th>\n",
       "      <th>params_ent_coef</th>\n",
       "      <th>params_gamma</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_n_epochs</th>\n",
       "      <th>params_n_steps</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-69.7</td>\n",
       "      <td>2021-07-30 16:43:01.270852</td>\n",
       "      <td>2021-07-30 16:43:17.682306</td>\n",
       "      <td>00:00:16.411454</td>\n",
       "      <td>0.199523</td>\n",
       "      <td>6.774265e-08</td>\n",
       "      <td>0.965521</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>1.004371</td>\n",
       "      <td>375.961929</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-365.1</td>\n",
       "      <td>2021-07-30 16:43:17.714981</td>\n",
       "      <td>2021-07-30 16:43:45.369104</td>\n",
       "      <td>00:00:27.654123</td>\n",
       "      <td>0.168132</td>\n",
       "      <td>3.488590e-05</td>\n",
       "      <td>0.936340</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>3.222209</td>\n",
       "      <td>1150.443913</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>2021-07-30 16:43:45.407851</td>\n",
       "      <td>2021-07-30 16:44:34.835498</td>\n",
       "      <td>00:00:49.427647</td>\n",
       "      <td>0.344423</td>\n",
       "      <td>2.034752e-06</td>\n",
       "      <td>0.937947</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>22.986353</td>\n",
       "      <td>168.052912</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>2021-07-30 16:44:34.866702</td>\n",
       "      <td>2021-07-30 16:44:55.407044</td>\n",
       "      <td>00:00:20.540342</td>\n",
       "      <td>0.239389</td>\n",
       "      <td>1.057548e-03</td>\n",
       "      <td>0.984803</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>2.751369</td>\n",
       "      <td>95.540450</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>2021-07-30 16:44:55.435277</td>\n",
       "      <td>2021-07-30 16:45:18.965896</td>\n",
       "      <td>00:00:23.530619</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>1.395738e-06</td>\n",
       "      <td>0.969958</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4.128084</td>\n",
       "      <td>45.899706</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-221.7</td>\n",
       "      <td>2021-07-30 16:45:19.001041</td>\n",
       "      <td>2021-07-30 16:45:39.779680</td>\n",
       "      <td>00:00:20.778639</td>\n",
       "      <td>0.204749</td>\n",
       "      <td>7.772477e-04</td>\n",
       "      <td>0.990752</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>1.494602</td>\n",
       "      <td>284.360187</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>-138.0</td>\n",
       "      <td>2021-07-30 16:45:39.817929</td>\n",
       "      <td>2021-07-30 16:46:14.635955</td>\n",
       "      <td>00:00:34.818026</td>\n",
       "      <td>0.304674</td>\n",
       "      <td>1.092933e-02</td>\n",
       "      <td>0.950058</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>8.150697</td>\n",
       "      <td>37.375459</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-51.5</td>\n",
       "      <td>2021-07-30 16:46:14.669138</td>\n",
       "      <td>2021-07-30 16:46:43.976614</td>\n",
       "      <td>00:00:29.307476</td>\n",
       "      <td>0.282713</td>\n",
       "      <td>4.258884e-05</td>\n",
       "      <td>0.904336</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7.377646</td>\n",
       "      <td>1971.999042</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-9.2</td>\n",
       "      <td>2021-07-30 16:46:44.008058</td>\n",
       "      <td>2021-07-30 16:48:31.672601</td>\n",
       "      <td>00:01:47.664543</td>\n",
       "      <td>0.114702</td>\n",
       "      <td>1.746112e-03</td>\n",
       "      <td>0.911550</td>\n",
       "      <td>0.675674</td>\n",
       "      <td>19.177987</td>\n",
       "      <td>17.360869</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-124.3</td>\n",
       "      <td>2021-07-30 16:48:31.702212</td>\n",
       "      <td>2021-07-30 16:48:53.001079</td>\n",
       "      <td>00:00:21.298867</td>\n",
       "      <td>0.381342</td>\n",
       "      <td>3.246627e-08</td>\n",
       "      <td>0.950959</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>1.646246</td>\n",
       "      <td>36.887792</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>2021-07-30 16:48:53.035595</td>\n",
       "      <td>2021-07-30 16:50:29.003212</td>\n",
       "      <td>00:01:35.967617</td>\n",
       "      <td>0.112359</td>\n",
       "      <td>2.482035e-05</td>\n",
       "      <td>0.922781</td>\n",
       "      <td>0.122654</td>\n",
       "      <td>47.406735</td>\n",
       "      <td>2047.595577</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-149.2</td>\n",
       "      <td>2021-07-30 16:50:29.034215</td>\n",
       "      <td>2021-07-30 16:50:53.768758</td>\n",
       "      <td>00:00:24.734543</td>\n",
       "      <td>0.174004</td>\n",
       "      <td>1.463472e-04</td>\n",
       "      <td>0.928131</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>1.897679</td>\n",
       "      <td>594.841941</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>-144.3</td>\n",
       "      <td>2021-07-30 16:50:53.814276</td>\n",
       "      <td>2021-07-30 16:51:18.242447</td>\n",
       "      <td>00:00:24.428171</td>\n",
       "      <td>0.170908</td>\n",
       "      <td>7.119646e-02</td>\n",
       "      <td>0.999550</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>1.136905</td>\n",
       "      <td>743.253103</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>-422.9</td>\n",
       "      <td>2021-07-30 16:51:18.276009</td>\n",
       "      <td>2021-07-30 16:51:45.272039</td>\n",
       "      <td>00:00:26.996030</td>\n",
       "      <td>0.223295</td>\n",
       "      <td>4.717648e-06</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>4.107394</td>\n",
       "      <td>1119.907275</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>-326.2</td>\n",
       "      <td>2021-07-30 16:51:45.316712</td>\n",
       "      <td>2021-07-30 16:52:12.691324</td>\n",
       "      <td>00:00:27.374612</td>\n",
       "      <td>0.145556</td>\n",
       "      <td>1.611173e-06</td>\n",
       "      <td>0.935174</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>4.206898</td>\n",
       "      <td>1139.610038</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>-310.3</td>\n",
       "      <td>2021-07-30 16:52:12.737676</td>\n",
       "      <td>2021-07-30 16:52:41.333521</td>\n",
       "      <td>00:00:28.595845</td>\n",
       "      <td>0.245742</td>\n",
       "      <td>8.753732e-06</td>\n",
       "      <td>0.964890</td>\n",
       "      <td>0.021004</td>\n",
       "      <td>4.311247</td>\n",
       "      <td>1283.080310</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-219.9</td>\n",
       "      <td>2021-07-30 16:52:41.376205</td>\n",
       "      <td>2021-07-30 16:53:21.212161</td>\n",
       "      <td>00:00:39.835956</td>\n",
       "      <td>0.215930</td>\n",
       "      <td>2.078588e-07</td>\n",
       "      <td>0.917394</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>11.872895</td>\n",
       "      <td>707.659332</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>-253.4</td>\n",
       "      <td>2021-07-30 16:53:21.255984</td>\n",
       "      <td>2021-07-30 16:53:51.377707</td>\n",
       "      <td>00:00:30.121723</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>1.639559e-04</td>\n",
       "      <td>0.946365</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>3.390736</td>\n",
       "      <td>2047.584001</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-30 16:53:51.419841</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.145960</td>\n",
       "      <td>5.796271e-06</td>\n",
       "      <td>0.976633</td>\n",
       "      <td>0.050185</td>\n",
       "      <td>2.374582</td>\n",
       "      <td>1220.710062</td>\n",
       "      <td>RUNNING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number  value             datetime_start          datetime_complete  \\\n",
       "0        0  -69.7 2021-07-30 16:43:01.270852 2021-07-30 16:43:17.682306   \n",
       "1        1 -365.1 2021-07-30 16:43:17.714981 2021-07-30 16:43:45.369104   \n",
       "2        2 -103.0 2021-07-30 16:43:45.407851 2021-07-30 16:44:34.835498   \n",
       "3        3   -9.6 2021-07-30 16:44:34.866702 2021-07-30 16:44:55.407044   \n",
       "4        4  -87.5 2021-07-30 16:44:55.435277 2021-07-30 16:45:18.965896   \n",
       "5        5 -221.7 2021-07-30 16:45:19.001041 2021-07-30 16:45:39.779680   \n",
       "6        6 -138.0 2021-07-30 16:45:39.817929 2021-07-30 16:46:14.635955   \n",
       "7        7  -51.5 2021-07-30 16:46:14.669138 2021-07-30 16:46:43.976614   \n",
       "8        8   -9.2 2021-07-30 16:46:44.008058 2021-07-30 16:48:31.672601   \n",
       "9        9 -124.3 2021-07-30 16:48:31.702212 2021-07-30 16:48:53.001079   \n",
       "10      10   -9.3 2021-07-30 16:48:53.035595 2021-07-30 16:50:29.003212   \n",
       "11      11 -149.2 2021-07-30 16:50:29.034215 2021-07-30 16:50:53.768758   \n",
       "12      12 -144.3 2021-07-30 16:50:53.814276 2021-07-30 16:51:18.242447   \n",
       "13      13 -422.9 2021-07-30 16:51:18.276009 2021-07-30 16:51:45.272039   \n",
       "14      14 -326.2 2021-07-30 16:51:45.316712 2021-07-30 16:52:12.691324   \n",
       "15      15 -310.3 2021-07-30 16:52:12.737676 2021-07-30 16:52:41.333521   \n",
       "16      16 -219.9 2021-07-30 16:52:41.376205 2021-07-30 16:53:21.212161   \n",
       "17      17 -253.4 2021-07-30 16:53:21.255984 2021-07-30 16:53:51.377707   \n",
       "18      18    NaN 2021-07-30 16:53:51.419841                        NaT   \n",
       "\n",
       "          duration  params_clip_range  params_ent_coef  params_gamma  \\\n",
       "0  00:00:16.411454           0.199523     6.774265e-08      0.965521   \n",
       "1  00:00:27.654123           0.168132     3.488590e-05      0.936340   \n",
       "2  00:00:49.427647           0.344423     2.034752e-06      0.937947   \n",
       "3  00:00:20.540342           0.239389     1.057548e-03      0.984803   \n",
       "4  00:00:23.530619           0.385800     1.395738e-06      0.969958   \n",
       "5  00:00:20.778639           0.204749     7.772477e-04      0.990752   \n",
       "6  00:00:34.818026           0.304674     1.092933e-02      0.950058   \n",
       "7  00:00:29.307476           0.282713     4.258884e-05      0.904336   \n",
       "8  00:01:47.664543           0.114702     1.746112e-03      0.911550   \n",
       "9  00:00:21.298867           0.381342     3.246627e-08      0.950959   \n",
       "10 00:01:35.967617           0.112359     2.482035e-05      0.922781   \n",
       "11 00:00:24.734543           0.174004     1.463472e-04      0.928131   \n",
       "12 00:00:24.428171           0.170908     7.119646e-02      0.999550   \n",
       "13 00:00:26.996030           0.223295     4.717648e-06      0.999673   \n",
       "14 00:00:27.374612           0.145556     1.611173e-06      0.935174   \n",
       "15 00:00:28.595845           0.245742     8.753732e-06      0.964890   \n",
       "16 00:00:39.835956           0.215930     2.078588e-07      0.917394   \n",
       "17 00:00:30.121723           0.281250     1.639559e-04      0.946365   \n",
       "18             NaT           0.145960     5.796271e-06      0.976633   \n",
       "\n",
       "    params_learning_rate  params_n_epochs  params_n_steps     state  \n",
       "0               0.000158         1.004371      375.961929  COMPLETE  \n",
       "1               0.002574         3.222209     1150.443913  COMPLETE  \n",
       "2               0.004233        22.986353      168.052912  COMPLETE  \n",
       "3               0.017578         2.751369       95.540450  COMPLETE  \n",
       "4               0.000045         4.128084       45.899706  COMPLETE  \n",
       "5               0.002455         1.494602      284.360187  COMPLETE  \n",
       "6               0.000034         8.150697       37.375459  COMPLETE  \n",
       "7               0.000010         7.377646     1971.999042  COMPLETE  \n",
       "8               0.675674        19.177987       17.360869  COMPLETE  \n",
       "9               0.000258         1.646246       36.887792  COMPLETE  \n",
       "10              0.122654        47.406735     2047.595577  COMPLETE  \n",
       "11              0.001375         1.897679      594.841941  COMPLETE  \n",
       "12              0.013752         1.136905      743.253103  COMPLETE  \n",
       "13              0.000787         4.107394     1119.907275  COMPLETE  \n",
       "14              0.000399         4.206898     1139.610038  COMPLETE  \n",
       "15              0.021004         4.311247     1283.080310  COMPLETE  \n",
       "16              0.000812        11.872895      707.659332  COMPLETE  \n",
       "17              0.004413         3.390736     2047.584001  COMPLETE  \n",
       "18              0.050185         2.374582     1220.710062   RUNNING  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A large part of the code in this file was sourced from the rl-baselines-zoo library on GitHub.\n",
    "In particular, the library provides a great parameter optimization set for the PPO2 algorithm,\n",
    "as well as a great example implementation using optuna.\n",
    "Source: https://github.com/araffin/rl-baselines-zoo/blob/master/utils/hyperparams_opt.py\n",
    "'''\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder, SubprocVecEnv\n",
    "#from wandb.integration.sb3 import WandbCallback\n",
    "#import wandb\n",
    "\n",
    "#env = Template_Gym()\n",
    "#from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "#from stable_baselines.gail import ExpertDataset\n",
    "\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%y%m%d%H%M%S')\n",
    "config = {\"policy_type\": \"MlpPolicy\", \"total_timesteps\": 25000}\n",
    "experiment_name = f\"PPO_{int(time.time())}\"\n",
    "class Optimization():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.reward_strategy = 'sortino2'\n",
    "        #self.input_data_file = 'data/coinbase_hourly.csv'\n",
    "        self.params_db_file = 'sqlite:///params.db'\n",
    "\n",
    "        # number of parallel jobs\n",
    "        self.n_jobs = 1\n",
    "        # maximum number of trials for finding the best hyperparams\n",
    "        self.n_trials = 100\n",
    "        #number of test episodes per trial\n",
    "        self.n_test_episodes = 10\n",
    "        # number of evaluations for pruning per trial\n",
    "        self.n_evaluations = 10\n",
    "\n",
    "\n",
    "        #self.df = pd.read_csv(input_data_file)\n",
    "        #self.df = df.drop(['Symbol'], axis=1)\n",
    "        #self.df = df.sort_values(['Date'])\n",
    "        #self.df = add_indicators(df.reset_index())\n",
    "\n",
    "        #self.train_len = int(len(df) * 0.8)\n",
    "\n",
    "        #self.df = df[:train_len]\n",
    "\n",
    "        #self.validation_len = int(train_len * 0.8)\n",
    "        #self.train_df = df[:validation_len]\n",
    "        #self.test_df = df[validation_len:]\n",
    "\n",
    "    def make_env(self, env_id, rank, seed=0, eval=False):\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "    \n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "        def _init():\n",
    "            self.eval= eval\n",
    "            env = gym.make(\"CartPole-v1\")\n",
    "            env.seed(seed + rank)\n",
    "            return env\n",
    "        #set_global_seeds(seed)\n",
    "        return _init\n",
    "    #def make_env():\n",
    "        #env = gym.make(\"CartPole-v1\")\n",
    "        #env = Monitor(env)  # record stats such as returns\n",
    "        #return env\n",
    "    \n",
    "\n",
    "    def optimize_envs(self, trial):\n",
    "        return {\n",
    "            'reward_func': self.reward_strategy,\n",
    "            'forecast_len': int(trial.suggest_loguniform('forecast_len', 1, 200)),\n",
    "            'confidence_interval': trial.suggest_uniform('confidence_interval', 0.7, 0.99),\n",
    "        }\n",
    "\n",
    "\n",
    "    def optimize_ppo2(self,trial):\n",
    "        return {\n",
    "            'n_steps': int(trial.suggest_loguniform('n_steps', 16, 2048)),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.),\n",
    "            'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n",
    "            'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "            'n_epochs': int(trial.suggest_loguniform('n_epochs', 1, 48)),\n",
    "            #'lam': trial.suggest_uniform('lam', 0.8, 1.)\n",
    "        }\n",
    "\n",
    "\n",
    "    def optimize_agent(self,trial):\n",
    "        #self.env_params = self.optimize_envs(trial)\n",
    "        env_id = \"default\"\n",
    "        num_e = 1  # Number of processes to use\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        #self.train_env = DummyVecEnv([lambda: env()])\n",
    "        self.train_env = gym.make('CartPole-v1')\n",
    "        #self.train_env = VecNormalize(self.train_env, norm_obs=True, norm_reward=True)\n",
    "        #self.test_env = DummyVecEnv([lambda: env()])\n",
    "        self.test_env = env = gym.make('CartPole-v1')\n",
    "        #self.test_env = VecNormalize(self.train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "        self.model_params = self.optimize_ppo2(trial)\n",
    "        \n",
    "        self.model = PPO(config[\"policy_type\"], self.train_env, verbose=0, tensorboard_log=Path(\"./tensorboard2\").name, **self.model_params)\n",
    "        #self.model = PPO2(CustomPolicy_2, self.env, verbose=0, learning_rate=1e-4, nminibatches=1, tensorboard_log=\"./min1\" )\n",
    "\n",
    "        last_reward = -np.finfo(np.float16).max\n",
    "        #evaluation_interval = int(len(train_df) / self.n_evaluations)\n",
    "        evaluation_interval = 3000\n",
    "\n",
    "        for eval_idx in range(self.n_evaluations):\n",
    "            try:\n",
    "                self.model.learn(evaluation_interval)\n",
    "            except AssertionError:\n",
    "                raise\n",
    "\n",
    "            rewards = []\n",
    "            n_episodes, reward_sum = 0, 0.0\n",
    "\n",
    "            obs = self.test_env.reset()\n",
    "            while n_episodes < self.n_test_episodes:\n",
    "                action, _ = self.model.predict(obs)\n",
    "                obs, reward, done, _ = self.test_env.step(action)\n",
    "                reward_sum += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(reward_sum)\n",
    "                    reward_sum = 0.0\n",
    "                    n_episodes += 1\n",
    "                    obs = self.test_env.reset()\n",
    "\n",
    "            last_reward = np.mean(rewards)\n",
    "            trial.report(-1 * last_reward, eval_idx)\n",
    "\n",
    "            #if trial.should_prune(eval_idx):\n",
    "                #raise optuna.structs.TrialPruned()\n",
    "\n",
    "        return -1 * last_reward\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        study_name = 'ppo299_' + self.reward_strategy\n",
    "        #study = optuna.create_study(\n",
    "            #study_name=study_name, storage=self.params_db_file, load_if_exists=True)\n",
    "        study = optuna.create_study(\n",
    "            study_name=study_name, storage=self.params_db_file, load_if_exists=True)\n",
    "        try:\n",
    "            study.optimize(self.optimize_agent, n_trials=self.n_trials, n_jobs=self.n_jobs)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        print('Number of finished trials: ', len(study.trials))\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print('Value: ', trial.value)\n",
    "\n",
    "        print('Params: ')\n",
    "        for key, value in trial.params.items():\n",
    "            print('    {}: {}'.format(key, value))\n",
    "\n",
    "        return study.trials_dataframe()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "run = Optimization()\n",
    "run.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: adaptationai (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.11.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO_1627624999</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/adaptationai/test\" target=\"_blank\">https://wandb.ai/adaptationai/test</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/adaptationai/test/runs/21xb4vt2\" target=\"_blank\">https://wandb.ai/adaptationai/test/runs/21xb4vt2</a><br/>\n",
       "                Run data is saved locally in <code>/home/adaptation/Documents/github/adaptationio/Shaman-AI/wandb/run-20210730_153320-21xb4vt2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/PPO_1627624999/PPO_1\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-0-to-step-200.mp4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 380      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-2000-to-step-2200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 27.4         |\n",
      "|    ep_rew_mean          | 27.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 367          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075403433 |\n",
      "|    clip_fraction        | 0.091        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | -0.0011      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.15         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 53.8         |\n",
      "------------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-4000-to-step-4200.mp4\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35          |\n",
      "|    ep_rew_mean          | 35          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009798685 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0674      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 35.3        |\n",
      "-----------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-6000-to-step-6200.mp4\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 45.1       |\n",
      "|    ep_rew_mean          | 45.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00972058 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.6       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    value_loss           | 52.3       |\n",
      "----------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-8000-to-step-8200.mp4\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-10000-to-step-10200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60.7         |\n",
      "|    ep_rew_mean          | 60.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 367          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087480815 |\n",
      "|    clip_fraction        | 0.0817       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | 0.311        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 63.3         |\n",
      "------------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-12000-to-step-12200.mp4\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73          |\n",
      "|    ep_rew_mean          | 73          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007000612 |\n",
      "|    clip_fraction        | 0.0455      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 55.3        |\n",
      "-----------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-14000-to-step-14200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.3         |\n",
      "|    ep_rew_mean          | 93.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 378          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047686975 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.651        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 41.6         |\n",
      "------------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-16000-to-step-16200.mp4\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | 109         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007871492 |\n",
      "|    clip_fraction        | 0.0773      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.01        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    value_loss           | 56.9        |\n",
      "-----------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-18000-to-step-18200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 127          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 384          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036996587 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 57.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-20000-to-step-20200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 145          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 384          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050595244 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.25         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000633    |\n",
      "|    value_loss           | 46.5         |\n",
      "------------------------------------------\n",
      "Saving video to /home/adaptation/Documents/github/adaptationio/Shaman-AI/videos/rl-video-step-22000-to-step-22200.mp4\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 162          |\n",
      "|    ep_rew_mean          | 162          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 386          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038821264 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.547       |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0064      |\n",
      "|    value_loss           | 70           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a4fa092e1405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgradient_save_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmodel_save_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"models/{experiment_name}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     ),\n\u001b[1;32m     43\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tried to pass invalid video frame, marking as broken: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your frame has data type {}, but we require uint8 (i.e. RGB values from 0-255).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "\n",
    "config = {\"policy_type\": \"MlpPolicy\", \"total_timesteps\": 25000}\n",
    "experiment_name = f\"PPO_{int(time.time())}\"\n",
    "\n",
    "# Initialise a W&B run\n",
    "wandb.init(\n",
    "    name=experiment_name,\n",
    "    project=\"test\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "env = VecVideoRecorder(env, \"videos\",\n",
    "    record_video_trigger=lambda x: x % 2000 == 0, video_length=200)\n",
    "\n",
    "model = PPO(config[\"policy_type\"], env, verbose=1,\n",
    "    tensorboard_log=f\"runs/{experiment_name}\")\n",
    "\n",
    "# Add the WandbCallback \n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_freq=1000,\n",
    "        model_save_path=f\"models/{experiment_name}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Sampler for PPO hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128, 256, 512])\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    lr_schedule = \"constant\"\n",
    "    # Uncomment to enable learning rate schedule\n",
    "    # lr_schedule = trial.suggest_categorical('lr_schedule', ['linear', 'constant'])\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
    "    clip_range = trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4])\n",
    "    n_epochs = trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0, 1)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\"])\n",
    "    # Uncomment for gSDE (continuous actions)\n",
    "    # log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    # Uncomment for gSDE (continuous action)\n",
    "    # sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 8, 16, 32, 64, 128, 256])\n",
    "    # Orthogonal initialization\n",
    "    ortho_init = False\n",
    "    # ortho_init = trial.suggest_categorical('ortho_init', [False, True])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-2a0fba73efa3>, line 246)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2a0fba73efa3>\"\u001b[0;36m, line \u001b[0;32m246\u001b[0m\n\u001b[0;31m    self.model = PPO(config[\"policy_type\"], self.train_env, verbose=1, tensorboard_log=verbose=1, tensorboard_log=f\"runs\", **self.model_params )\u001b[0m\n\u001b[0m                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A large part of the code in this file was sourced from the rl-baselines-zoo library on GitHub.\n",
    "In particular, the library provides a great parameter optimization set for the PPO2 algorithm,\n",
    "as well as a great example implementation using optuna.\n",
    "Source: https://github.com/araffin/rl-baselines-zoo/blob/master/utils/hyperparams_opt.py\n",
    "'''\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import argparse\n",
    "from functools import partial\n",
    "import time\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder, SubprocVecEnv, VecNormalize \n",
    "#from stable_baselines3 import PPO\n",
    "#from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "#from wandb.integration.sb3 import WandbCallback\n",
    "#import wandb\n",
    "\n",
    "\n",
    "#from stable_baselines.common.policies import MlpLnLstmPolicy, LstmPolicy, CnnPolicy, MlpPolicy\n",
    "#from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv,VecNormalize \n",
    "#from stable_baselines3.common import set_global_seeds\n",
    "#from stable_baselines import ACKTR, PPO2, SAC\n",
    "#from stable_baselines.deepq import DQN\n",
    "#from stable_baselines.deepq.policies import FeedForwardPolicy\n",
    "#from ..env import Template_Gym\n",
    "#from ..common import CustomPolicy, CustomPolicy_2, CustomLSTMPolicy, CustomPolicy_4, CustomPolicy_3, CustomPolicy_5\n",
    "#from ..common import PairList, PairConfig, PairsConfigured\n",
    "#env = Template_Gym()\n",
    "#from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "#from stable_baselines.gail import ExpertDataset\n",
    "\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%y%m%d%H%M%S')\n",
    "#pc = PairsConfigured()\n",
    "\n",
    "config = {\"policy_type\": \"MlpPolicy\", \"total_timesteps\": 25000}\n",
    "experiment_name = f\"PPO_{int(time.time())}\"\n",
    "\n",
    "\n",
    "class Optimization():\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.reward_strategy = 'Name it'\n",
    "        #self.input_data_file = 'data/coinbase_hourly.csv'\n",
    "        self.params_db_file = 'sqlite:///params.db'\n",
    "\n",
    "        # number of parallel jobs\n",
    "        self.n_jobs = 1\n",
    "        # maximum number of trials for finding the best hyperparams\n",
    "        self.n_trials = 100\n",
    "        #number of test episodes per trial\n",
    "        self.n_test_episodes = 10\n",
    "        # number of evaluations for pruning per trial\n",
    "        self.n_evaluations = 10\n",
    "        self.config = config\n",
    "\n",
    "        #self.df = pd.read_csv(input_data_file)\n",
    "        #self.df = df.drop(['Symbol'], axis=1)\n",
    "        #self.df = df.sort_values(['Date'])\n",
    "        #self.df = add_indicators(df.reset_index())\n",
    "\n",
    "        #self.train_len = int(len(df) * 0.8)\n",
    "\n",
    "        #self.df = df[:train_len]\n",
    "\n",
    "        #self.validation_len = int(train_len * 0.8)\n",
    "        #self.train_df = df[:validation_len]\n",
    "        #self.test_df = df[validation_len:]\n",
    "\n",
    "    #def make_env(self, env_id, rank, seed=0, eval=False):\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "    \n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "        #def _init():\n",
    "            #self.config = config\n",
    "            #self.eval= eval\n",
    "            #env = gym.make(config[\"env_name\"])\n",
    "            #env = Monitor(env)\n",
    "            #env = Template_Gym(config=self.config, eval=self.eval)\n",
    "            #env.seed(seed + rank)\n",
    "            #return env\n",
    "        #set_global_seeds(seed)\n",
    "        #return _init\n",
    "    #def make_env(env_id, rank, seed=0):\n",
    "        \"\"\"\n",
    "        Utility function for multiprocessed env.\n",
    "\n",
    "        :param env_id: (str) the environment ID\n",
    "        :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "        :param seed: (int) the inital seed for RNG\n",
    "        :param rank: (int) index of the subprocess\n",
    "        \"\"\"\n",
    "        #def _init():\n",
    "            #env = gym.make(env_id)\n",
    "            #env.seed(seed + rank)\n",
    "            #return env\n",
    "        #set_random_seed(seed)\n",
    "        #return _init\n",
    "    \n",
    "    def make_env():\n",
    "        env = gym.make(config[\"env_name\"])\n",
    "        env = Monitor(env)  # record stats such as returns\n",
    "        return env\n",
    "    \n",
    "    # Categorical parameter\n",
    "    #optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])\n",
    "\n",
    "    # Int parameter\n",
    "    #num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    # Uniform parameter\n",
    "    #dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)\n",
    "\n",
    "    # Loguniform parameter\n",
    "    #learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Discrete-uniform parameter\n",
    "    #drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1)\n",
    "    def optimize_envs(self, trial):\n",
    "        return {\n",
    "            'reward_func': self.reward_strategy,\n",
    "            'forecast_len': int(trial.suggest_loguniform('forecast_len', 1, 200)),\n",
    "            'confidence_interval': trial.suggest_uniform('confidence_interval', 0.7, 0.99),\n",
    "        }\n",
    "\n",
    "    def optimize_config(self, trial):\n",
    "        return {\n",
    "            'sl': trial.suggest_loguniform('sl', 1.0, 10.0),\n",
    "            'tp': trial.suggest_loguniform('tp', 1.0 ,10.0)\n",
    "            \n",
    "        }\n",
    "\n",
    "    def optimize_ppo2(self,trial):\n",
    "        return {\n",
    "            #'n_steps': int(trial.suggest_int('n_steps', 16, 2048)),\n",
    "            #'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),\n",
    "            #'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.),\n",
    "            #'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n",
    "            #'cliprange': trial.suggest_uniform('cliprange', 0.1, 0.4),\n",
    "            #'noptepochs': int(trial.suggest_int('noptepochs', 1, 48)),\n",
    "            #'lam': trial.suggest_uniform('lam', 0.8, 1.)\n",
    "            \n",
    "            \n",
    "            'batch_size': trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128, 256, 512]),\n",
    "            'n_steps': int(trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048])),\n",
    "            'gamma': trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999]),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-5, 1),\n",
    "            #'lr_schedule' = \"constant\"\n",
    "            # Uncomment to enable learning rate schedule\n",
    "            # lr_schedule = trial.suggest_categorical('lr_schedule', ['linear', 'constant'])\n",
    "            'ent_coef': trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1),\n",
    "            'clip_range': trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4]),\n",
    "            'n_epochs': trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20]),\n",
    "            'gae_lambda': trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0]),\n",
    "            'max_grad_norm': trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5]),\n",
    "            'vf_coef': trial.suggest_uniform(\"vf_coef\", 0, 1)\n",
    "            #'net_arch' = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\"])\n",
    "            # Uncomment for gSDE (continuous actions)\n",
    "            # log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "            # Uncomment for gSDE (continuous action)\n",
    "            # sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 8, 16, 32, 64, 128, 256])\n",
    "            # Orthogonal initialization\n",
    "            #ortho_init = False\n",
    "            # ortho_init = trial.suggest_categorical('ortho_init', [False, True])\n",
    "            # activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "            #'activation_fn': trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "        }\n",
    "\n",
    "    def optimize_lstm(self, trial):\n",
    "        return {\n",
    "            'lstm': trial.suggest_categorical('optimizer', ['lstm', 'mlp'])\n",
    "            \n",
    "        }\n",
    "    def ob_types(self, trial):\n",
    "        return {\n",
    "            'lstm': trial.suggest_categorical('optimizer', ['lstm', 'mlp'])\n",
    "            \n",
    "        }\n",
    "\n",
    "\n",
    "    def optimize_agent(self,trial):\n",
    "        run = wandb.init(\n",
    "        project=\"sb3\",\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "        save_code=True,  # optional\n",
    "        )\n",
    "        #self.env_params = self.optimize_envs(trial)\n",
    "        env_id = \"default\"+str()\n",
    "        num_e = self.n_jobs  # Number of processes to use\n",
    "        #self.config_param = self.optimize_config(trial)\n",
    "        #self.config.sl = self.config_param['sl']\n",
    "        #self.config.sl = self.config_param['tp']\n",
    "        #self.model_type = self.optimize_lstm(trial)\n",
    "        #self.model_type = self.model_type['lstm']\n",
    "        #self.model_type = \"mlp\"\n",
    "        #if self.model_type == 'mlp':\n",
    "            #self.policy = CustomPolicy_5\n",
    "        #else:\n",
    "             #self.policy = MlpPolicy\n",
    "        #self.train_env = SubprocVecEnv([self.make_env(env_id+str('train'), i) for i in range(num_e)])\n",
    "        #SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
    "        #self.train_env = SubprocVecEnv([self.make_env(env_id, i, eval=False) for i in range(num_e)])\n",
    "        #self.train_env = VecNormalize(self.train_env, norm_obs=True, norm_reward=True)\n",
    "        #self.test_env = SubprocVecEnv([self.make_env(env_id+str(\"test\"), i) for i in range(num_e)])\n",
    "        #self.test_env = SubprocVecEnv([self.make_env(env_id, i, eval=True) for i in range(num_e)])\n",
    "        #self.test_env = VecNormalize(self.test_env, norm_obs=True, norm_reward=True)\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        self.train_env = DummyVecEnv([lambda: env])\n",
    "        self.train_env = VecVideoRecorder(self.train_env, \"videos\", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)\n",
    "        #self.train_env = DummyVecEnv([env])\n",
    "        #self.train_env = VecNormalize(self.train_env, norm_obs=True, norm_reward=True)\n",
    "        self.test_env = DummyVecEnv([lambda: env])\n",
    "        self.test_env = VecVideoRecorder(self.test_env, \"videos\", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)\n",
    "        #self.test_env = DummyVecEnv([env])\n",
    "        try:\n",
    "            self.test_env.load_running_average(\"saves\")\n",
    "            self.train_env.load_running_average(\"saves\")\n",
    "        except:\n",
    "            print('cant load')\n",
    "        self.model_params = self.optimize_ppo2(trial)\n",
    "        self.model = PPO(config[\"policy_type\"], self.train_env, verbose=1, tensorboard_log=f\"runs\", **self.model_params )\n",
    "        #self.model = PPO2(CustomPolicy_2, self.env, verbose=0, learning_rate=1e-4, nminibatches=1, tensorboard_log=\"./min1\" )\n",
    "\n",
    "        last_reward = -np.finfo(np.float16).max\n",
    "        #evaluation_interval = int(len(train_df) / self.n_evaluations)\n",
    "        evaluation_interval = 3500\n",
    "\n",
    "        for eval_idx in range(self.n_evaluations):\n",
    "            try:\n",
    "                #self.model.learn(evaluation_interval)\n",
    "                self.model.learn(\n",
    "                    total_timesteps=evaluation_interval,\n",
    "                    callback=WandbCallback(gradient_save_freq=100,\n",
    "                    model_save_path=f\"models/{run.id}\",\n",
    "                    verbose=2,\n",
    "                    ),\n",
    "                )\n",
    "                #self.test_env.save_running_average(\"saves\")\n",
    "                #self.train_env.save_running_average(\"saves\")\n",
    "            except:\n",
    "                print('did not work')\n",
    "\n",
    "            rewards = []\n",
    "            n_episodes, reward_sum = 0, 0.0\n",
    "            print('Eval')\n",
    "            obs = self.test_env.reset()\n",
    "            #state = None\n",
    "            #done = [False for _ in range(self.env.num_envs)]\n",
    "            while n_episodes < self.n_test_episodes:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, _ = self.test_env.step(action)\n",
    "                reward_sum += reward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(reward_sum)\n",
    "                    reward_sum = 0.0\n",
    "                    n_episodes += 1\n",
    "                    obs = self.test_env.reset()\n",
    "\n",
    "            last_reward = np.mean(rewards)\n",
    "            trial.report(-1 * last_reward, eval_idx)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.structs.TrialPruned()\n",
    "        run.finish()\n",
    "        return -1 * last_reward\n",
    "\n",
    "\n",
    "    def optimize(self, config):\n",
    "        self.config = config\n",
    "        study_name = 'ppo2_single_ready'\n",
    "        study_name = 'ppo2_single_ready_nosltp'\n",
    "        study_name = 'ppo2_single_ready_nosltp_all_yeah'\n",
    "        study_name = 'ppo2_eur_gbp_op'\n",
    "        study_name = 'ppo2_gbp_chf_op'\n",
    "        study_name = 'ppo2_gbp_chf_h1_new1'\n",
    "        study_name = 'ppo2_gbp_chf_h4_r_new11'\n",
    "        study_name = 'ppo2_gbp_chf_h4_r_withvolfixed'\n",
    "        study_name = 'ppo2_gbp_chf_h4_r_withvolclosefix212'\n",
    "        study_name = 'ppo2_gbp_chf_h4_loged_sortinonew'\n",
    "        study_name = 'AUD_CHF_4H_SELL_C5_NEW'\n",
    "        study_name = 'wandb'\n",
    "        study = optuna.create_study(\n",
    "            study_name=study_name, storage=self.params_db_file, load_if_exists=True)\n",
    "\n",
    "        try:\n",
    "            study.optimize(self.optimize_agent, n_trials=self.n_trials, n_jobs=self.n_jobs)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        print('Number of finished trials: ', len(study.trials))\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(trial.number)\n",
    "        print('Value: ', trial.value)\n",
    "\n",
    "        print('Params: ')\n",
    "        for key, value in trial.params.items():\n",
    "            print('    {}: {}'.format(key, value))\n",
    "\n",
    "        return study.trials_dataframe()\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #optimize()\n",
    "run = Optimization(config)\n",
    "run.optimize(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
